# Load libraries
library(dplyr)
# 1. Get the data
d <- read.csv("data/AppendixII.csv")
# 2. Clean the data (also saved in output/AppendixII_cleaned,csv)
source("analyses/clean_orca.R")
#Create a new column that combines Pod and Likely Pod columna and removes spaces
d$Pod.cl<-d$Pod
#Always use Likely Pod column, when it is not blank:
d$Pod.cl[d$LikelyPod!="" & d$LikelyPod!=" "]<-d$LikelyPod[d$LikelyPod!="" & d$LikelyPod!=" "]
#only using fishing areas in Washington's Salish Sea
#d<-d[d$FishArea %in% c("01","02","03","04","05","06","07","09","10","11","12","13","81","82"),]#not sure where 17, 18, 19, 20, 28, 29 are...need to find out. also, where is 42583,42584
#keep canadian sites in upper salish sea
d<-d[d$FishArea %in% c("01","02","03","04","05","06","07","09","10","11","12","13","81","82","19C","18C","29C","20C"),]#not sure where 17, 18, 19, 20, 28, 29 are...need to find out. also, where is 42583,42584
#remove sites with no fishing area:
d<-d[!d$FishArea %in% c(""),]
#Only use fishing areas that have atleast 4 years with >20 observations:
#11 fishing areas with >5
# 8 fishing areas with >10
# 8 fishing areas with >20
tab.fa.yr<-table(d$FishArea,d$Year)
tab.fa.yr[tab.fa.yr < 20] <- 0
tab.fa.yr[tab.fa.yr >= 20] <- 1
sites.touse<-rownames(tab.fa.yr)[rowSums(tab.fa.yr)>4]
d<-d[d$FishArea %in% c(sites.touse),]
#Add week and day of year (day)
d$day<-strftime(strptime(paste(d$Month, d$Day, d$Year, sep="."),format= "%m.%d.%Y"),format= "%j")
d$week<-strftime(strptime(paste(d$Month, d$Day, d$Year, sep="."),format= "%m.%d.%Y"), format = "%V")#new weeks start on mondays
d$J<-0
d$J[grep("J",d$Pod.cl)]<- 1
d$K<-0
d$K[grep("K",d$Pod.cl)]<- 1
d$L<-0
d$L[grep("L",d$Pod.cl)]<- 1
d$SRKW<-0
d$SRKW[grep("SR",d$Pod.cl)]<- 1
d$SRKW[d$J==1|d$K==1|d$L==1]<- 1
d$Orcas<-1
#only data after 1978
d<-d[d$Year>1978,]
# Add a column for total number of observations
#of all SRKWs during each week of each year year to estimate detection
#d$wkyrfa<-paste(d$Year, d$week,d$FishArea,sep="_")#if we decide to add FishArea to detection estimates for model
#d$wkyr<-paste(d$Year, d$week,sep="_")
#unique(d$FishArea)
d$yrdayfa<-paste(d$Year, d$day,d$FishArea,sep="_")
#Raw detection ratios:
obs = aggregate(Orcas ~yrdayfa, data = d,sum)
# presence of each pod
js = aggregate(J ~yrdayfa, data = d,sum)
ks = aggregate(K~yrdayfa, data = d,sum)
ls = aggregate(L~yrdayfa, data = d,sum)
srs = aggregate(SRKW~yrdayfa, data = d,sum)
det<-cbind(js,ks[,2],ls[,2],srs[,2],obs[2])
colnames(det)[2:6]<-c("Jobs","Kobs","Lobs","AllSRobs","nrep")
det$year<-substr(det$yrdayfa,1,4)
det$day<-substr(det$yrdayfa,6,8)
det$fa<-substr(det$yrdayfa,10,nchar(det$yrdayfa))
#assign to ps (puget sound) or uss (upper salish sea) using fishing area
det$region<-"ps"
det$region[det$fa=="07"|det$fa=="06"|det$fa=="02"|det$fa=="04"|det$fa=="19C"|det$fa== "18C"|det$fa=="20C"]<-"uss"
det$region[det$fa=="01"|det$fa=="02"|det$fa=="03"]<-"oc"#outer coast
det$site<-as.numeric(as.factor(det$fa))
det$day<-as.numeric(det$day)
det$year<-as.numeric(det$year)
#Add a column for "season" and divide up by season.Not sure if these are best...
#start with winter vs summer
det$season<-NA
det$season[det$day>274|det$day<60]<-1#winter (Oct 1-March)
det$season[det$day>121 & det$day<274]<-2#summer (May-Oct 1)
#add an "orca year" which runs Oct 1-Sept 31
det$orcayear<-det$year
det$orcayear[which(det$day>273)]<-det$year[which(det$day>273)]+1
det$daysaftsept30<-NA
det$daysaftsept30[which(det$day>273 & det$day<367)]<-det$day[which(det$day>273 & det$day<367)]-273
det$daysaftsept30[which(det$day<274)]<-det$day[which(det$day<274)]+93#this should actually vary depending on whether or not it is a leap year
jdet<-subset(det,select=c(nrep,Jobs,site,day,orcayear,daysaftsept30,season,region))
jdet <- jdet[apply(jdet, 1, function(x) all(!is.na(x))),] # only keep rows of all not na
#jdet<- jdet[apply(jdet,1,function(row) all(row!=0)),]
kdet<-subset(det,select=c(nrep,Kobs,site,day,orcayear,daysaftsept30,season,region))
kdet <- kdet[apply(kdet, 1, function(x) all(!is.na(x))),] # only keep rows of all not na
ldet<-subset(det,select=c(nrep,Lobs,site,day,orcayear,daysaftsept30,season,region))
ldet <- ldet[apply(ldet, 1, function(x) all(!is.na(x))),] # only keep rows of all not na
srdet<-subset(det,select=c(nrep,AllSRobs,site,day,orcayear,daysaftsept30,season,region))
srdet <- srdet[apply(srdet, 1, function(x) all(!is.na(x))),] # only keep rows of all not na
colnames(srdet)[2]<-colnames(jdet)[2]<-colnames(kdet)[2]<-colnames(ldet)[2]<-"ndet"
colnames(srdet)[5]<-colnames(jdet)[5]<-colnames(kdet)[5]<-colnames(ldet)[5]<-"year"
write.csv(srdet,"analyses/output/allsr_dat.csv",row.names = FALSE)
write.csv(ldet,"analyses/output/l_dat.csv",row.names = FALSE)
write.csv(kdet,"analyses/output/k_dat.csv",row.names = FALSE)
write.csv(jdet,"analyses/output/j_dat.csv",row.names = FALSE)
#Make some plots to look at the data
#summarize by sitings per day of each season and pod over time
# Choose the data you want to look at:
pods<-c("J","K","L","SR")
seasons<-c(1,2)
for(i in 1:length(pods)){
for(j in 1:length(seasons)){
pod=pods[i]#options= J,K,L, SR (all 3 pods)
season=seasons[j]#options= 1(winter) or 2(summer)
if(season==1){region="ps"}#options=upper salish sea (uss) or puget sound (ps)
if(season==2){region="uss"}
if(pod=="J"){dat<-jdet}
if(pod=="K"){dat<-kdet}
if(pod=="L"){dat<-ldet}
if(pod=="SR"){dat<-srdet}
#restrict to season
dat<-dat[dat$season==season,]
#if winter  (season 1), then days= days ater sept 30
if(season=="1"){
dat<-subset(dat,select=c(nrep,ndet,site, daysaftsept30,year,season,region))
colnames(dat)[4]<-"day"
}
#choose region
dat<-dat[dat$region==region,]
dim(dat)
#find doy with max number of obs for each year
dayobs<-aggregate(dat$ndet,list(dat$year,dat$day),sum)
colnames(dayobs)<-c("year","day","obs")
dayeffort<-aggregate(dat$nrep,list(dat$year,dat$day),sum)
colnames(dayeffort)<-c("year","day","effort")
propobs<-left_join(dayobs,dayeffort)
propobs$propobs<-propobs$obs/propobs$effort
#find which day this occurs on
maxobs<-aggregate(dayobs$obs,list(dayobs$year),max)
colnames(maxobs)<-c("year","obs")
peakobs<-dplyr::inner_join(maxobs, dayobs)
maxprop<-aggregate(propobs$propobs,list(propobs$year),max)
colnames(maxprop)<-c("year","propobs")
peakpropdoy<-dplyr::inner_join(maxprop,propobs)
#plot peak obs by year
pdf(file=paste("analyses/figures/SR/orcaphen_1976_2017",region,season,pod,"rawdat.pdf", sep="_"),width=7,height=6)
### plot peak number of obs over all years
#quartz()
par(mfrow=c(1,1),mai=c(1,1,1,0.5))
x=peakobs$year
y=peakobs$day
plot(x,y,xlab="Year",ylab="DOY of peak # sightings",main=paste("Peak Number of Sightings","\n",pod[i]," Pod",season[j]),
ylim=c(min(peakobs$day),max(peakobs$day)),pch=21,type="p", bg="gray")
dev.off()
print(pods[i]);  print(seasons[j]);
print(summary(lm(y~x)))
print(summary(lmer(y~x+(1|as.factor(x)))))
# plot peak proportion of obs over all years
#quartz()
pdf(file=paste("analyses/figures/SR/orcaphen_1976_2017",region,season,pod,"prop.pdf", sep="_"),width=7,height=6)
par(mfrow=c(1,1),mai=c(1,1,1,0.5))
x=peakpropdoy$year
y=peakpropdoy$day
plot(x,y,xlab="Year",ylab="DOY of peak proportion sightings",main=paste("Peak Proportion of Sightings","\n",pod[i]," Pod",season[j]),
ylim=c(min(peakpropdoy$day),max(peakpropdoy$day)),pch=21,type="p", bg="gray")
dev.off()
print(pods[i]);print(seasons[j])
print(summary(lm(y~x)))
print(summary(lmer(y~x+(1|as.factor(x)))))
years<-unique(dat$year)
for(yr in min(years):max(years)){
pdf(file=paste("analyses/figures/", pod,"/rawdat",yr,"_",season,"_",region,".pdf", sep=""),width=8,height=6)
yrdat<-dat[dat$year==yr,]
x=yrdat$day
y=yrdat$ndet
plot(x,y,main=paste("Number sightings","\n",pod," Pod",yr),
ylim=c(min(yrdat$nrep, na.rm = TRUE),max(yrdat$nrep, na.rm = TRUE)),pch=16,type="p", col="black")
dev.off()
}
}
}
#Now make figures for whole year
# Choose the data you want to look at:
pods<-c("J","K","L","SR")
for(i in 1:length(pods)){
pod=pods[i]
if(pod=="J"){dat<-jdet}
if(pod=="K"){dat<-kdet}
if(pod=="L"){dat<-ldet}
if(pod=="SR"){dat<-srdet}
#find doy with max number of obs for each year
dayobs<-aggregate(dat$ndet,list(dat$year,dat$day),sum)
colnames(dayobs)<-c("year","day","obs")
years<-unique(dayobs$year)
for(yr in min(years):max(years)){
pdf(file=paste("analyses/figures/", pod,"/rawdat",yr,"_yearroundallsites_",".pdf", sep=""),width=8,height=6)
yrdat<-dayobs[dayobs$year==yr,]
x=yrdat$day
y=yrdat$obs
plot(x,y,main=paste("Number sightings","\n",pod," Pod",yr),
ylim=c(min(yrdat$nrep, na.rm = TRUE),max(yrdat$nrep, na.rm = TRUE)),xlab="doy",ylab="# sightings",pch=16,type="p", col="black")
dev.off()
}
}
##################################################################
# Orca phenology occupancy model, based on the
# Worked example to run the model presented in Strebel et al., 2014
# (Study of phenology by flexible estimation and modeling of seasonal detectability peaks)
# Ailene Ettinger, ailene.ettinger@noaa.gov
# (modifed from code of  Nicolas Strebel, nicolas_strebel@gmx.ch)
# Start Date:	November 27, 2018
# Title:	orca_run_occ_model
##################################################################
# Set working directory
setwd("~/Documents/GitHub/fishphen")
# Load libraries
library(R2jags)
library(scales)
# Choose the data you want:
pod="K"#options= J,K,L,SR
season="1"#options= 1(winter) or 2(summer)
region="ps"#options=upper salish sea (uss) or puget sound (ps)
# Choose the data you want:
pod="SR"#options= J,K,L,SR
season="1"#options= 1(winter) or 2(summer)
region="ps"#options=upper salish sea (uss) or puget sound (ps)
if(pod=="J"){dat<-read.csv("analyses/output/j_dat.csv",header=T)}
if(pod=="K"){dat<-read.csv("analyses/output/k_dat.csv",header=T)}
if(pod=="L"){dat<-read.csv("analyses/output/l_dat.csv",header=T)}
if(pod=="SR"){dat<-read.csv("analyses/output/allsr_dat.csv",header=T)}
#restrict to season
dat<-dat[dat$season==season,]
#if winter  (season 1), then days= days ater sept 30
if(season=="1"){
dat<-subset(dat,select=c(nrep,ndet,site, daysaftsept30,year,season,region))
colnames(dat)[4]<-"day"
}
#choose region
dat<-dat[dat$region==region,]
dim(dat)
# Specify model in BUGS language
sink("analyses/splinesSiteOcc S4.txt")
cat("
model {
### Define seasonal and annual patterns in detectability
for (m in 1:nyear) {
for (i in 1:n) {
logit(p[m,i]) <- lp[m,i]
lp[m,i] <- mfe[m,i]+mre[m,i]
mfe[m,i] <- a[m]*X[i,1]+b[m]*X[i,2]+c[m]*X[i,3]
mre[m,i]<-sum(n.mre[m,i,1:nknots])
for (k in 1:nknots) {
n.mre[m,i,k]<-b.k[m,k]*Z[i,k]
}
}
### Random regression coefficients corresponding to the truncated polynomial functions
for (k in 1:nknots) {
b.k[m,k] ~ dnorm(0,taub)
}
### Fixed regression coefficients corresponding to the 'plus' functions
a[m] ~ dnorm(0,0.01)
b[m] ~ dnorm(0,0.01)
c[m] ~ dnorm(0,0.01)
}
### precision for random regression coefficients corresponding to the truncated polynomial functions
taub~dgamma(1.0E-6,1.0E-6)
# Specify priors
for (k in 1:nyear) {
psi[k] ~ dunif(0, 1)
}
# Ecological submodel: Define state conditional on parameters
for (i in 1:nsite){
for (k in 1:nyear){
z[i,k] ~ dbern(psi[k])
}
}
# Observation model
for (i in 1:nobs){
muy[site[i],survey[i],year[i]] <- z[site[i],year[i]]*p[year[i],survey[i]]
y[i] ~ dbin(muy[site[i],survey[i],year[i]], nrep[i])
}
}
",fill = TRUE)
sink()
### The following procedure is based on the models presented in Crainiceanu et al. 2005 and in Gimenez et al. 2006
# Degree of splines
degree <- 2
# covariate
covariate<-as.numeric(scale(range(dat$day)[1]:range(dat$day)[2]))
# covariate length
n <- length(covariate)
# location of knots
nk<-round((max(dat$day)-min(dat$day)+1)/4)
nknots<-ifelse(nk<35,nk,35)
knots<-quantile(unique(covariate),seq(0,1,length=(nknots+2))[-c(1,(nknots+2))])
#Note: the maximum number of nots is 35. thus, the annual model (for which nk=92 in many cases, but it is restricted to 35 by default) differs in flexibility than the seasonal model
#perhaps better to extract the seasonal peaks after fitting the whole year of data
# fixed effects matrix
X<-NULL
for (l in 0:degree) {
X<-cbind(X,covariate^l)
}
# random coefficients matrix
Z_K<-(abs(outer(covariate,knots,"-")))^3
OMEGA_all<-(abs(outer(knots,knots,"-")))^3
svd.OMEGA_all<-svd(OMEGA_all)
sqrt.OMEGA_all<-t(svd.OMEGA_all$v %*% (t(svd.OMEGA_all$u)*sqrt(svd.OMEGA_all$d)))
Z<-t(solve(sqrt.OMEGA_all,t(Z_K)))
# Input data
dat$site <- factor(dat$site)#
dat$site <- droplevels(dat$site)
dat$site <- as.integer(dat$site)
site <- dat$site
survey <- dat$day-min(dat$day)+1
nobs <- length(unique(paste(dat$site,dat$day,dat$year)))
nrep <- dat$nrep
nsite <- length(unique(dat$site))
nyear <- length(unique(dat$year))
year <- as.numeric(factor(dat$year))
zst <- array(1, dim=c(nsite,nyear))
y <- dat$ndet
# Simulation parameters
#ni=15000; nc=2; nb=0; nt=10
ni=5000; nc=2; nb=1500; nt=1
# List input data
jags.data <- list("site","survey","nobs","nrep","nsite","nyear","year","nknots","n","X","Z","nc", "nb", "ni", "nt","zst","y")
# Inits function
f.inits <- function(){list(a=rep(0,nyear), b=rep(0,nyear), c=rep(0,nyear), z=zst)}
# specify the parameters to be monitored
parameters <- c("a","b","c","lp","psi","taub")
jags.out<-jags.parallel(jags.data,f.inits,parameters,"splinesSiteOcc S4.txt",nc,ni,nb,nt)
quartz()
plot(jags.out)
#Look at psi
out<-jags.out$BUGSoutput
jags.out$BUGSoutput$mean$psi#probability of presence (annual)
# Save model output
if(pod=="J" & season=="1"){save(out,file="jags.output/jpod out season1")}
if(pod=="J" & season=="2"){save(out,file="jags.output/jpod out season2")}
if(pod=="K" & season=="1"){save(out,file="jags.output/kpod out season1")}
if(pod=="K" & season=="2"){save(out,file="jags.output/kpod out season2")}
if(pod=="L" & season=="1"){save(out,file="jags.output/lpod out season1")}
if(pod=="L" & season=="2"){save(out,file="jags.output/lpod out season2")}
if(pod=="SR" & season=="1"){save(out,file="jags.output/allsrpods out season1")}
if(pod=="SR" & season=="2"){save(out,file="jags.output/allsrpods out season2")}
### get estimated date of peak detectability based on posterior distribution
# get date of peak detectability in each simulation
findmax.fn<-function(x) {
mean(which(x==max(x)))
}
lpmax<-array(data=NA,dim=c(out$n.sims,nyear))
dimnames(lpmax)<-list(c(1:out$n.sims),c(sort(unique(dat$year))))
for (xj in sort(unique(as.numeric(factor(dat$year))))) {
lpmax[,xj]<-apply(out$sims.array[,,paste("lp[",xj[1],",",1:(max(dat$day)-min(dat$day)+1),"]",sep="")],MARGIN=c(if(out$n.chains>1) 1:2 else 1),findmax.fn)
}
lpmax<-lpmax+min(dat$day)-1
lpmax[lpmax==max(dat$day)]<-NA
lpmax[lpmax==min(dat$day)]<-NA
#would like to Extract psi (probability of presence by day...)
dim(out$sims.list$psi)
# summarize estimates
ann.res<-array(NA, dim=c(max(dat$year)-min(dat$year)+1,3),dimnames=list(c(min(dat$year):max(dat$year)),c("mean","10%","90%")))
res<-apply(lpmax,c(2),mean,na.rm=T)
ann.res[names(res),"mean"]<-res
res<-apply(lpmax,c(2),quantile,probs=0.10,na.rm=T)
ann.res[names(res),"10%"]<-res
res<-apply(lpmax,c(2),quantile,probs=0.90,na.rm=T)
ann.res[names(res),"90%"]<-res
# get estimate of trend in date of peak detectability over years
do.lm<-function(x) {
lmres<-lm(x~as.numeric(names(x)))$coefficients
return(lmres)
}
r<-matrix(NA,dim(lpmax)[1],2)
for (o in 1:(dim(lpmax)[1])) {
# if(!is.na(sum(lpmax[o,]))) {
lm(lpmax[o,]~as.numeric(colnames(lpmax)))$coefficients->r[o,]
#}
}
slopevec<-as.vector(r[,2])
intercept<-mean(r[,1],na.rm=T)
slope<-mean(r[,2],na.rm=T)
intercept.10<-quantile(r[,1],c(0.10),na.rm=T)
intercept.90<-quantile(r[,1],c(0.90),na.rm=T)
slope.10<-quantile(r[,2],c(0.10),na.rm=T)
slope.90<-quantile(r[,2],c(0.90),na.rm=T)
#get first date when detectability is greater than 0.5
findfirst.fn<-function(x) {
min(which(plogis(x)>0.50), na.rm=TRUE)
}
#check:
#count.fn<-function(x) {
#  length(which(plogis(x)<0.10))
#}
firstlp<-array(data=NA,dim=c(out$n.sims,nyear))
dimnames(firstlp)<-list(c(1:out$n.sims),c(sort(unique(dat$year))))
for (xj in sort(unique(as.numeric(factor(dat$year))))) {
firstlp[,xj]<-apply(out$sims.array[,,paste("lp[",xj[1],",",1:(max(dat$day)-min(dat$day)+1),"]",sep="")],MARGIN=c(if(out$n.chains>1) 1:2 else 1),findfirst.fn)
}
firstlp<-firstlp+min(dat$day)-1
firstlp
firstlp[firstlp==max(dat$day)]<-NA
firstlp[firstlp==min(dat$day)]<-NA
firstlp[which(firstlp=="Inf")]<-NA
# summarize estimates
ann.first<-array(NA, dim=c(max(dat$year)-min(dat$year)+1,3),dimnames=list(c(min(dat$year):max(dat$year)),c("mean","10%","90%")))
first<-apply(firstlp,c(2),mean,na.rm=T)
ann.first[names(first),"mean"]<-first
first<-apply(firstlp,c(2),quantile,probs=0.10,na.rm=T)
ann.first[names(first),"10%"]<-first
first<-apply(firstlp,c(2),quantile,probs=0.90,na.rm=T)
ann.first[names(first),"90%"]<-first
# get estimate of trend in date of peak detectability over years
#firstlp<-as.numeric()
r.first<-matrix(NA,dim(firstlp)[1],2)
for (o in 1:(dim(firstlp)[1])) {
# if(!is.na(sum(lpmax[o,]))) {
y<-firstlp[o,]
y[y=="Inf"]<-NA
lm(y~as.numeric(colnames(firstlp)))$coefficients->r.first[o,]
#}
}
slopevec.first<-as.vector(r.first[,2])
intercept.first<-mean(r.first[,1],na.rm=T)
slope.first<-mean(r.first[,2],na.rm=T)
intercept.first.10<-quantile(r.first[,1],c(0.10),na.rm=T)
intercept.first.90<-quantile(r.first[,1],c(0.90),na.rm=T)
slope.first.10<-quantile(r.first[,2],c(0.10),na.rm=T)
slope.first.90<-quantile(r.first[,2],c(0.90),na.rm=T)
#get last date when detectability is greater than 0.5
findlast.fn<-function(x) {
max(which(plogis(x)>0.50), na.rm=TRUE)
}
#check:
#count.fn<-function(x) {
#  length(which(plogis(x)<0.50))
#}
lastlp<-array(data=NA,dim=c(out$n.sims,nyear))
dimnames(lastlp)<-list(c(1:out$n.sims),c(sort(unique(dat$year))))
for (xj in sort(unique(as.numeric(factor(dat$year))))) {
lastlp[,xj]<-apply(out$sims.array[,,paste("lp[",xj[1],",",1:(max(dat$day)-min(dat$day)+1),"]",sep="")],MARGIN=c(if(out$n.chains>1) 1:2 else 1),findlast.fn)
}
lastlp<-lastlp+min(dat$day)-1
lastlp[lastlp==max(dat$day)]<-NA
lastlp[lastlp==min(dat$day)]<-NA
lastlp[which(lastlp=="Inf")]<-NA
lastlp[which(lastlp=="-Inf")]<-NA
# summarize estimates
ann.last<-array(NA, dim=c(max(dat$year)-min(dat$year)+1,3),dimnames=list(c(min(dat$year):max(dat$year)),c("mean","10%","90%")))
last<-apply(lastlp,c(2),mean,na.rm=T)
ann.last[names(last),"mean"]<-last
last<-apply(lastlp,c(2),quantile,probs=0.10,na.rm=T)
ann.last[names(last),"10%"]<-last
last<-apply(lastlp,c(2),quantile,probs=0.90,na.rm=T)
ann.last[names(last),"90%"]<-last
# get estimate of trend in date of peak detectability over years
#firstlp<-as.numeric()
r.last<-matrix(NA,dim(lastlp)[1],2)
for (o in 1:(dim(lastlp)[1])) {
# if(!is.na(sum(lpmax[o,]))) {
y<-lastlp[o,]
y[y=="Inf"]<-NA
lm(y~as.numeric(colnames(lastlp)))$coefficients->r.last[o,]
#}
}
slopevec.last<-as.vector(r.last[,2])
intercept.last<-mean(r.last[,1],na.rm=T)
slope.last<-mean(r.last[,2],na.rm=T)
intercept.last.10<-quantile(r.last[,1],c(0.10),na.rm=T)
intercept.last.90<-quantile(r.last[,1],c(0.90),na.rm=T)
slope.last.10<-quantile(r.last[,2],c(0.10),na.rm=T)
slope.last.90<-quantile(r.last[,2],c(0.90),na.rm=T)
### Write results (in console if argument file is not specified in function cat)
if(season=="1"){
cat(paste("summary results",pod,region,season),"\n",
paste("annual change of activity peak:", round(mean(slopevec,na.rm=T),digits=2),"days"),
paste("confidence interval from", round(quantile(slopevec,0.10,na.rm=T),digits=2),
"to",round(quantile(slopevec,0.90,na.rm=T),digits=2)),
"\n","mean estimate of activity peak","as date",
as.character(as.Date(x=c(ann.res[,colnames(ann.res)=="mean"]),origin=c(paste(row.names(ann.res),"-09-30",sep="")))),"\n",
sep="\n","as days after sept 30",
paste(rownames(ann.res),round(ann.res[,"mean"])))
cat(paste("summary results",pod,region,season),"\n",
paste("annual change of first activity doy:", round(mean(slopevec.first,na.rm=T),digits=2),"days"),
paste("confidence interval from", round(quantile(slopevec.first,0.10,na.rm=T),digits=2),
"to",round(quantile(slopevec.first,0.90,na.rm=T),digits=2)),
"\n","mean estimate of first activity doy","as date",
as.character(as.Date(x=c(ann.first[,colnames(ann.first)=="mean"]),origin=c(paste(row.names(ann.first),"-09-30",sep="")))),"\n",
sep="\n","as days after sept 30",
paste(rownames(ann.first),round(ann.first[,"mean"])))
cat(paste("summary results",pod,region,season),"\n",
paste("annual change of last activity doy:", round(mean(slopevec.first,na.rm=T),digits=2),"days"),
paste("confidence interval from", round(quantile(slopevec.first,0.10,na.rm=T),digits=2),
"to",round(quantile(slopevec.first,0.90,na.rm=T),digits=2)),
"\n","mean estimate of last activity doy","as date",
as.character(as.Date(x=c(ann.last[,colnames(ann.last)=="mean"]),origin=c(paste(row.names(ann.last),"-09-30",sep="")))),"\n",
sep="\n","as days after sept 30",
paste(rownames(ann.last),round(ann.last[,"mean"])))
}
### Write results (in console if argument file is not specified in function cat)
if(season=="1"){
cat(paste("summary results",pod,region,season),"\n",
paste("annual change of activity peak:", round(mean(slopevec,na.rm=T),digits=2),"days"),
paste("confidence interval from", round(quantile(slopevec,0.10,na.rm=T),digits=2),
"to",round(quantile(slopevec,0.90,na.rm=T),digits=2)),
"\n","mean estimate of activity peak","as date",
as.character(as.Date(x=c(ann.res[,colnames(ann.res)=="mean"]),origin=c(paste(row.names(ann.res),"-09-30",sep="")))),"\n",
sep="\n","as days after sept 30",
paste(rownames(ann.res),round(ann.res[,"mean"])))
cat(paste("summary results",pod,region,season),"\n",
paste("annual change of first activity doy:", round(mean(slopevec.first,na.rm=T),digits=2),"days"),
paste("confidence interval from", round(quantile(slopevec.first,0.10,na.rm=T),digits=2),
"to",round(quantile(slopevec.first,0.90,na.rm=T),digits=2)),
"\n","mean estimate of first activity doy","as date",
as.character(as.Date(x=c(ann.first[,colnames(ann.first)=="mean"]),origin=c(paste(row.names(ann.first),"-09-30",sep="")))),"\n",
sep="\n","as days after sept 30",
paste(rownames(ann.first),round(ann.first[,"mean"])))
cat(paste("summary results",pod,region,season),"\n",
paste("annual change of last activity doy:", round(mean(slopevec.last,na.rm=T),digits=2),"days"),
paste("confidence interval from", round(quantile(slopevec.last,0.10,na.rm=T),digits=2),
"to",round(quantile(slopevec.last,0.90,na.rm=T),digits=2)),
"\n","mean estimate of last activity doy","as date",
as.character(as.Date(x=c(ann.last[,colnames(ann.last)=="mean"]),origin=c(paste(row.names(ann.last),"-09-30",sep="")))),"\n",
sep="\n","as days after sept 30",
paste(rownames(ann.last),round(ann.last[,"mean"])))
}
